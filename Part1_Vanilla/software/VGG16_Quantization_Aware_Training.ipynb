{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): ReLU(inplace=True)\n",
      "    (31): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (32): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (33): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (34): ReLU(inplace=True)\n",
      "    (35): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (36): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (37): ReLU(inplace=True)\n",
      "    (38): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (39): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (40): ReLU(inplace=True)\n",
      "    (41): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (42): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 512\n",
    "model_name = \"VGG16_quant_project_part1\"   #\"Resnet20_quant\"\n",
    "model = VGG16_quant_project_part1()\n",
    "\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [80, 120]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1           \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81716c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "def prune_model(model, amount=0.8, method='mixed'):\n",
    "    \"\"\"\n",
    "    Applies pruning to the model.\n",
    "    method: \n",
    "        'unstructured': Prunes individual weights (L1 norm).\n",
    "        'structured': Prunes entire channels/filters (L1 norm).\n",
    "        'mixed': Combines structured and unstructured pruning to achieve the target sparsity.\n",
    "                 (e.g., 40% structured + remaining unstructured to reach target)\n",
    "    \"\"\"\n",
    "    print(f\"Applying {method} pruning with target amount={amount}...\")\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (QuantConv2d, nn.Conv2d, nn.Linear)):\n",
    "            if method == 'unstructured':\n",
    "                prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            elif method == 'structured':\n",
    "                # Prune output channels (dim=0)\n",
    "                prune.ln_structured(module, name='weight', amount=amount, n=1, dim=0)\n",
    "            elif method == 'mixed':\n",
    "                # Strategy: Apply 50% structured pruning, then apply unstructured pruning \n",
    "                # to the remaining weights to reach the total target amount.\n",
    "                # Formula: total_sparsity = 1 - (1 - s) * (1 - u)\n",
    "                # If target is 0.8 and s is 0.5:\n",
    "                # 0.8 = 1 - (0.5) * (1 - u) => 0.2 = 0.5 * (1 - u) => 0.4 = 1 - u => u = 0.6\n",
    "                \n",
    "                s_amount = 0.4 # Fixed structured amount\n",
    "                if amount > s_amount:\n",
    "                    u_amount = 1 - (1 - amount) / (1 - s_amount)\n",
    "                    \n",
    "                    # Apply structured first\n",
    "                    prune.ln_structured(module, name='weight', amount=s_amount, n=1, dim=0)\n",
    "                    # Apply unstructured on top\n",
    "                    prune.l1_unstructured(module, name='weight', amount=u_amount)\n",
    "                else:\n",
    "                    # If target is less than fixed structured amount, just use structured\n",
    "                    prune.ln_structured(module, name='weight', amount=amount, n=1, dim=0)\n",
    "\n",
    "    # Calculate and print global sparsity\n",
    "    total_zeros = 0\n",
    "    total_params = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (QuantConv2d, nn.Conv2d, nn.Linear)):\n",
    "            if hasattr(module, 'weight'):\n",
    "                # Check for mask\n",
    "                if prune.is_pruned(module):\n",
    "                    zeros = torch.sum(module.weight == 0).item()\n",
    "                else:\n",
    "                    zeros = torch.sum(module.weight == 0).item()\n",
    "                total_zeros += zeros\n",
    "                total_params += module.weight.nelement()\n",
    "    \n",
    "    if total_params > 0:\n",
    "        print(f\"Global sparsity achieved: {total_zeros/total_params*100:.2f}%\")\n",
    "\n",
    "# Example usage:\n",
    "# prune_model(model, amount=0.8, method='mixed')\n",
    "\n",
    "# Note: To make pruning permanent (remove masks and update weights permanently):\n",
    "# for name, module in model.named_modules():\n",
    "#     if isinstance(module, (QuantConv2d, nn.Conv2d, nn.Linear)):\n",
    "#         if prune.is_pruned(module):\n",
    "#             prune.remove(module, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5211e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_feature_maps(model, input_image, layer_idx=0, title=\"Feature Maps\"):\n",
    "    \"\"\"\n",
    "    Visualizes the feature maps of a specific convolutional layer in the model.\n",
    "    Args:\n",
    "        model: The PyTorch model.\n",
    "        input_image: A single input image tensor [1, C, H, W].\n",
    "        layer_idx: The index of the convolutional layer in model.features to visualize.\n",
    "        title: Title for the plot.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        activations.append(output)\n",
    "        \n",
    "    # Register hook to the specified convolutional layer index\n",
    "    count = 0\n",
    "    handle = None\n",
    "    # Iterate through features to find the conv layer\n",
    "    for m in model.features:\n",
    "        if isinstance(m, (nn.Conv2d, QuantConv2d)):\n",
    "            if count == layer_idx:\n",
    "                handle = m.register_forward_hook(hook)\n",
    "                break\n",
    "            count += 1\n",
    "            \n",
    "    if handle is None:\n",
    "        print(f\"Convolutional layer index {layer_idx} not found\")\n",
    "        return\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        model(input_image)\n",
    "        \n",
    "    handle.remove()\n",
    "    \n",
    "    # Plot\n",
    "    if len(activations) > 0:\n",
    "        fmaps = activations[0][0].cpu().numpy() # [C, H, W]\n",
    "        num_plots = min(16, fmaps.shape[0]) # Plot up to 16 feature maps\n",
    "        \n",
    "        rows = 2\n",
    "        cols = 8\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(16, 4))\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "        \n",
    "        for i in range(rows * cols):\n",
    "            ax = axes[i // cols, i % cols]\n",
    "            if i < num_plots:\n",
    "                # Use gray_r colormap so 0 (inactive) appears white, and high values appear black/dark\n",
    "                ax.imshow(fmaps[i], cmap='gray_r')\n",
    "            ax.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No activations captured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "weight_decay = 1e-6\n",
    "epochs = 1000\n",
    "best_prec = 0\n",
    "# momentum = 0.9\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1).cuda()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=epochs,  # 500\n",
    "    eta_min=1e-10,  # final LR\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296fdaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 9162/10000 (92%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fdir = 'result/'+str(model_name)+'/model_best.pth.tar'\n",
    "\n",
    "checkpoint = torch.load(fdir)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "200d0ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved-hook index -> module name (in order):\n",
      "0 : features.0\n",
      "1 : features.2\n",
      "2 : features.3\n",
      "3 : features.5\n",
      "4 : features.6\n",
      "5 : features.7\n",
      "6 : features.9\n",
      "7 : features.10\n",
      "8 : features.12\n",
      "9 : features.13\n",
      "10 : features.14\n",
      "11 : features.16\n",
      "12 : features.17\n",
      "13 : features.19\n",
      "14 : features.20\n",
      "15 : features.22\n",
      "16 : features.23\n",
      "17 : features.24\n",
      "18 : features.26\n",
      "19 : features.27\n",
      "20 : features.28\n",
      "21 : features.29\n",
      "22 : features.30\n",
      "23 : features.31\n",
      "24 : features.32\n",
      "25 : features.34\n",
      "26 : features.35\n",
      "27 : features.37\n",
      "28 : features.38\n",
      "29 : features.40\n",
      "30 : features.41\n",
      "\n",
      "Modules that expose weight_q (quantized conv layers):\n",
      "  - features.0\n",
      "  - features.0.weight_quant\n",
      "  - features.3\n",
      "  - features.3.weight_quant\n",
      "  - features.7\n",
      "  - features.7.weight_quant\n",
      "  - features.10\n",
      "  - features.10.weight_quant\n",
      "  - features.14\n",
      "  - features.14.weight_quant\n",
      "  - features.17\n",
      "  - features.17.weight_quant\n",
      "  - features.20\n",
      "  - features.20.weight_quant\n",
      "  - features.24\n",
      "  - features.24.weight_quant\n",
      "  - features.27\n",
      "  - features.27.weight_quant\n",
      "  - features.29\n",
      "  - features.29.weight_quant\n",
      "  - features.32\n",
      "  - features.32.weight_quant\n",
      "  - features.35\n",
      "  - features.35.weight_quant\n",
      "  - features.38\n",
      "  - features.38.weight_quant\n",
      "\n",
      "Hook indices for quantized modules (if present in hook_map):\n",
      "features.0 -> hook indices: [0]\n",
      "features.0.weight_quant -> hook indices: []\n",
      "features.3 -> hook indices: [2]\n",
      "features.3.weight_quant -> hook indices: []\n",
      "features.7 -> hook indices: [5]\n",
      "features.7.weight_quant -> hook indices: []\n",
      "features.10 -> hook indices: [7]\n",
      "features.10.weight_quant -> hook indices: []\n",
      "features.14 -> hook indices: [10]\n",
      "features.14.weight_quant -> hook indices: []\n",
      "features.17 -> hook indices: [12]\n",
      "features.17.weight_quant -> hook indices: []\n",
      "features.20 -> hook indices: [14]\n",
      "features.20.weight_quant -> hook indices: []\n",
      "features.24 -> hook indices: [17]\n",
      "features.24.weight_quant -> hook indices: []\n",
      "features.27 -> hook indices: [19]\n",
      "features.27.weight_quant -> hook indices: []\n",
      "features.29 -> hook indices: [21]\n",
      "features.29.weight_quant -> hook indices: []\n",
      "features.32 -> hook indices: [24]\n",
      "features.32.weight_quant -> hook indices: []\n",
      "features.35 -> hook indices: [26]\n",
      "features.35.weight_quant -> hook indices: []\n",
      "features.38 -> hook indices: [28]\n",
      "features.38.weight_quant -> hook indices: []\n"
     ]
    }
   ],
   "source": [
    "# Pre-hook to save inputs\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []   # list of (name, module_in) for pre-hooks\n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "\n",
    "save_output = SaveOutput()\n",
    "hook_map = []   # keeps the module name for each saved output\n",
    "\n",
    "def make_pre_hook(name):\n",
    "    def hook(module, module_in, module_out=None):\n",
    "        # store (module_name, module_in tensor)\n",
    "        save_output.outputs.append((name, module_in))\n",
    "        hook_map.append(name)\n",
    "    return hook\n",
    "\n",
    "# register named pre-hooks only for relevant layer types\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (torch.nn.Conv2d, torch.nn.MaxPool2d, torch.nn.ReLU)):\n",
    "        module.register_forward_pre_hook(make_pre_hook(name))\n",
    "\n",
    "# run a single batch to populate save_output\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "save_output.clear()\n",
    "images, labels = next(iter(testloader))\n",
    "images = images.to(device)\n",
    "_ = model(images)\n",
    "\n",
    "# print mapping of saved outputs\n",
    "print(\"Saved-hook index -> module name (in order):\")\n",
    "for idx, nm in enumerate(hook_map):\n",
    "    print(idx, \":\", nm)\n",
    "\n",
    "# show modules that have weight_q (quantized convs) and find their hook indices\n",
    "print(\"\\nModules that expose weight_q (quantized conv layers):\")\n",
    "quant_names = []\n",
    "for name, m in model.named_modules():\n",
    "    if hasattr(m, 'weight_q'):\n",
    "        print(\"  -\", name)\n",
    "        quant_names.append(name)\n",
    "\n",
    "print(\"\\nHook indices for quantized modules (if present in hook_map):\")\n",
    "for qn in quant_names:\n",
    "    indices = [i for i, nm in enumerate(hook_map) if nm == qn]\n",
    "    print(qn, \"-> hook indices:\", indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target layer: features.27 (In: 8, Out: 8)\n",
      "clipping threshold weight alpha: 2.374000, activation alpha: 4.564000\n",
      "None\n",
      "Weight Int shape: torch.Size([8, 8, 3, 3])\n",
      "Weight Int Min: -7.0, Max: 7.0\n",
      "Found input for features.27\n",
      "Input Int shape: torch.Size([1, 8, 4, 4])\n",
      "Input Int Min: 0.0, Max: 13.0\n",
      "Saved activation_tile0.txt\n",
      "Generating weights for 3x3 convolution (9 files)...\n",
      "Saved 9 weight files.\n",
      "Saved out.txt\n"
     ]
    }
   ],
   "source": [
    "# Find x_int and w_int for the 8*8 convolution layer\n",
    "# Search for the layer with 8 input channels and 8 output channels\n",
    "target_layer = None\n",
    "for name, m in model.named_modules():\n",
    "    # Check for Conv2d or QuantConv2d\n",
    "    if hasattr(m, 'in_channels') and hasattr(m, 'out_channels'):\n",
    "        if m.in_channels == 8 and m.out_channels == 8:\n",
    "            target_layer = name\n",
    "            print(f\"Found target layer: {name} (In: {m.in_channels}, Out: {m.out_channels})\")\n",
    "            break\n",
    "\n",
    "if target_layer is None:\n",
    "    print(\"Error: Could not find a layer with 8 input and 8 output channels. Defaulting to features.27\")\n",
    "    target_layer = 'features.27'\n",
    "\n",
    "# 1. Get Weights for 8x8 Conv\n",
    "mod = dict(model.named_modules())[target_layer]\n",
    "\n",
    "w_bit = 4\n",
    "if hasattr(mod, 'weight_q'):\n",
    "    weight_q = mod.weight_q.detach()\n",
    "else:\n",
    "    weight_q = mod.weight.detach()\n",
    "\n",
    "print(mod.show_params())\n",
    "w_alpha = 2.374\n",
    "w_delta = w_alpha / ((2 ** (w_bit-1))-1)\n",
    "weight_int = torch.round(weight_q / w_delta)\n",
    "print(\"Weight Int shape:\", weight_int.shape) # Should be [8, 8, 3, 3]\n",
    "print(f\"Weight Int Min: {weight_int.min()}, Max: {weight_int.max()}\")\n",
    "\n",
    "# 2. Get Input for 8x8 Conv\n",
    "x_int = None\n",
    "for name, val in save_output.outputs:\n",
    "    if name == target_layer:\n",
    "        x = val[0][0:1].detach()\n",
    "        print(f\"Found input for {target_layer}\")\n",
    "        break\n",
    "\n",
    "if x is not None:\n",
    "    x_bit = 4\n",
    "    x_alpha = 4.564\n",
    "    x_delta = x_alpha / ((2 ** x_bit)-1)\n",
    "    \n",
    "    # Quantize Input\n",
    "    act_quant_fn = act_quantization(x_bit)\n",
    "    x_q = act_quant_fn(x, x_alpha)\n",
    "    x_int = torch.round(x_q / x_delta)\n",
    "    print(\"Input Int shape:\", x_int.shape) # Should be [Batch, 8, 4, 4]\n",
    "    print(f\"Input Int Min: {x_int.min()}, Max: {x_int.max()}\")\n",
    "\n",
    "if x_int is not None and weight_int is not None:\n",
    "    # Prepare files\n",
    "    f_a = open('activation_tile0.txt', 'w')\n",
    "    f_o = open('out.txt', 'w')\n",
    "\n",
    "    # Write Headers\n",
    "    header = '#col0row7[msb-lsb],col0row6[msb-lsb],....,col0row0[msb-lsb]#\\n'\n",
    "    header += '#col1row7[msb-lsb],col1row6[msb-lsb],....,col1row0[msb-lsb]#\\n'\n",
    "    header += '#................#\\n'\n",
    "    f_a.write(header) \n",
    "    f_o.write(header)\n",
    "\n",
    "    bit_precision = 4\n",
    "    \n",
    "    # Pad Input for 3x3 convolution (padding=1)\n",
    "    # x_int is [Batch, 8, 4, 4]\n",
    "    x_padded = F.pad(x_int, (1, 1, 1, 1), \"constant\", 0) # [Batch, 8, 6, 6]\n",
    "    \n",
    "    # --- Write Activation (ONCE) ---\n",
    "    # Flatten x_padded to [8, 36] (Channels, Spatial)\n",
    "    # We want to write 36 lines, each with 8 channels.\n",
    "    # The hardware expects the full 6x6 input map.\n",
    "    X_full = x_padded[0].reshape(8, -1) # [8, 36]\n",
    "    for t in range(X_full.size(1)): # 0 to 35\n",
    "        for c in range(X_full.size(0)): # 0 to 7\n",
    "             val = round(X_full[7-c, t].item()) # Descending channel order\n",
    "             if val < 0:\n",
    "                 val = val + (1 << bit_precision)\n",
    "             X_bin = '{0:04b}'.format(val)\n",
    "             for k in range(bit_precision):\n",
    "                 f_a.write(X_bin[k])\n",
    "        f_a.write('\\n')\n",
    "    f_a.close()\n",
    "    print(\"Saved activation_tile0.txt\")\n",
    "\n",
    "    print(\"Generating weights for 3x3 convolution (9 files)...\")\n",
    "    \n",
    "    # Loop over kernel positions (3x3)\n",
    "    # We iterate ky, kx from 0 to 2\n",
    "    kij = 0\n",
    "    for ky in range(3):\n",
    "        for kx in range(3):\n",
    "            # Open specific weight file for this kij\n",
    "            f_w = open(f'weight_itile0_otile0_kij{kij}.txt', 'w')\n",
    "            f_w.write(header)\n",
    "\n",
    "            # --- 1. Weights for this kernel position ---\n",
    "            # weight_int is [Out, In, Ky, Kx]\n",
    "            W = weight_int[:, :, ky, kx] # [Out, In]\n",
    "            \n",
    "            # Write W to f_w using HW_Code6 Logic\n",
    "            # W is [8, 8] (Out, In) -> (Col, Row)\n",
    "            for j in range(W.size(0)):  # column loop (outer)\n",
    "                for i in range(W.size(1)): # row loop (inner)\n",
    "                    val = round(W[j,7-i].item()) # Access Column j, Row (7-i)\n",
    "                    if val < 0:\n",
    "                        val = val + (1 << bit_precision)\n",
    "                    W_bin = '{0:04b}'.format(val)\n",
    "                    for k in range(bit_precision):\n",
    "                        f_w.write(W_bin[k])        \n",
    "                f_w.write('\\n')\n",
    "            f_w.close()\n",
    "            kij += 1\n",
    "            \n",
    "    print(\"Saved 9 weight files.\")\n",
    "\n",
    "    # --- 3. Expected Output ---\n",
    "    # Calculate full 3x3 convolution\n",
    "    out_int = F.conv2d(x_int, weight_int, padding=1)\n",
    "    out_relu = F.relu(out_int)\n",
    "    out_flat = out_relu[0].view(8, -1) # [8, 16] (Channels, Time)\n",
    "    \n",
    "    # Write out_flat to f_o using HW_Code6 Logic (psum.txt style)\n",
    "    # out_flat is [8, 16] (Col, Time) - Channels are Columns in output\n",
    "    bit_precision_out = 16\n",
    "    for t in range(out_flat.size(1)): # Loop over time steps\n",
    "        for c in range(out_flat.size(0)): # Loop over columns\n",
    "            # Access columns in descending order: 7, 6, ..., 0\n",
    "            val = round(out_flat[7-c, t].item())\n",
    "            \n",
    "            # 2's complement for negative numbers\n",
    "            if val < 0:\n",
    "                val = val + (1 << bit_precision_out)\n",
    "                \n",
    "            # Format to 16-bit binary string\n",
    "            val_bin = '{0:016b}'.format(val)\n",
    "            \n",
    "            # Write bits\n",
    "            for k in range(bit_precision_out):\n",
    "                f_o.write(val_bin[k])\n",
    "                \n",
    "        f_o.write('\\n') # New line after all columns for this time step\n",
    "        \n",
    "    print(\"Saved out.txt\")\n",
    "\n",
    "    f_o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally , save a integers version as reference\n",
    "f_w_int = open('weight_int.txt', 'w')\n",
    "# Loop over kernel positions (3x3)\n",
    "for ky in range(3):\n",
    "    for kx in range(3):\n",
    "        W = weight_int[:, :, ky, kx]\n",
    "        for j in range(W.size(0)):  # column loop (outer)\n",
    "            for i in range(W.size(1)): # row loop (inner)\n",
    "                val = round(W[j,7-i].item()) # Access Column j, Row (7-i)\n",
    "                f_w_int.write(f\"{val} \")\n",
    "            f_w_int.write('\\n')\n",
    "f_w_int.close()\n",
    "\n",
    "f_x_int = open('activation_int.txt', 'w')\n",
    "# Pad Input for 3x3 convolution (padding=1)\n",
    "x_padded = F.pad(x_int, (1, 1, 1, 1), \"constant\", 0) # [Batch, 8, 6, 6]\n",
    "\n",
    "# Write Activation (ONCE) - Full 6x6 map\n",
    "X_full = x_padded[0].reshape(8, -1) # [8, 36]\n",
    "for t in range(X_full.size(1)): # 0 to 35\n",
    "    for c in range(X_full.size(0)): # 0 to 7\n",
    "        val = round(X_full[7-c, t].item()) # Descending channel order\n",
    "        f_x_int.write(f\"{val} \")\n",
    "    f_x_int.write('\\n')\n",
    "f_x_int.close()\n",
    "\n",
    "f_o_int = open('out_int.txt', 'w')\n",
    "# Calculate full 3x3 convolution\n",
    "out_int = F.conv2d(x_int, weight_int, padding=1)\n",
    "out_relu = F.relu(out_int)\n",
    "out_flat = out_relu[0].view(8, -1) # [8, 16] (Channels, Time)\n",
    "for t in range(out_flat.size(1)): # Loop over time steps\n",
    "    for c in range(out_flat.size(0)): # Loop over columns\n",
    "        # Access columns in descending order: 7, 6, ..., 0\n",
    "        val = round(out_flat[7-c, t].item())\n",
    "        f_o_int.write(f\"{val} \")\n",
    "    f_o_int.write('\\n')\n",
    "f_o_int.close()\n",
    "print(\"Saved out_int.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92ea8931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAC-array simulation complete. Max abs error: 0\n",
      "Total mismatched entries: 0 / 128\n",
      "All simulated outputs exactly match the PyTorch reference.\n"
     ]
    }
   ],
   "source": [
    "# Simulate the 8x8 MAC array dataflow and compare with the PyTorch convolution result\n",
    "def simulate_mac_array(weight_tiles: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Emulates the weight-stationary 8x8 array accumulating psums across the 3x3 kernel passes.\"\"\"\n",
    "    assert weight_tiles is not None and input_tensor is not None, \"weight_int and x_int must already be computed\"\n",
    "    assert weight_tiles.shape == (8, 8, 3, 3), \"Expected weight tensor shaped [8, 8, 3, 3]\"\n",
    "    \n",
    "    x_pad = F.pad(input_tensor.to(torch.float32), (1, 1, 1, 1), \"constant\", 0)  # [1, 8, 6, 6]\n",
    "    psum = torch.zeros((weight_tiles.size(0), 16), dtype=torch.float32, device=x_pad.device)\n",
    "    weights_f32 = weight_tiles.to(torch.float32)\n",
    "    time_len = 4 * 4  # 16 positions per pass\n",
    "    \n",
    "    for ky in range(3):\n",
    "        for kx in range(3):\n",
    "            w_tile = weights_f32[:, :, ky, kx]                         # [8 out, 8 in]\n",
    "            x_slice = x_pad[:, :, ky:ky+4, kx:kx+4][0]                 # [8 in, 4, 4]\n",
    "            x_stream = x_slice.reshape(weights_f32.size(1), time_len)  # [8 in, 16 time]\n",
    "            psum += torch.matmul(w_tile, x_stream)                      # accumulate partial sums\n",
    "    return psum.to(torch.int32)\n",
    "\n",
    "mac_psum = simulate_mac_array(weight_int, x_int)\n",
    "mac_relu = torch.clamp(mac_psum, min=0)\n",
    "ref_out = torch.round(out_flat).to(torch.int32)\n",
    "\n",
    "diff = mac_relu - ref_out\n",
    "max_err = diff.abs().max().item()\n",
    "mismatches = (diff != 0).sum().item()\n",
    "\n",
    "print(f\"MAC-array simulation complete. Max abs error: {max_err}\")\n",
    "print(f\"Total mismatched entries: {mismatches} / {diff.numel()}\")\n",
    "if mismatches:\n",
    "    mismatch_idx = (diff != 0).nonzero(as_tuple=False)\n",
    "    sample = mismatch_idx[:5]\n",
    "    for idx in sample:\n",
    "        c, t = idx.tolist()\n",
    "        print(f\"  Channel {c}, time {t}: sim={mac_relu[c, t].item()} vs ref={ref_out[c, t].item()}\")\n",
    "else:\n",
    "    print(\"All simulated outputs exactly match the PyTorch reference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "550edb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing 6th Conv Layer Feature Maps BEFORE Pruning:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "sample_img = images[0].unsqueeze(0).cuda()\n",
    "\n",
    "print(\"Visualizing 6th Conv Layer Feature Maps BEFORE Pruning:\")\n",
    "visualize_feature_maps(model, sample_img, layer_idx=6, title=\"Before Pruning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc50718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying mixed pruning with target amount=0.7...\n",
      "Global sparsity achieved: 70.00%\n",
      "Sparsity level:  tensor(0.1940, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Apply Mixed Pruning of 70%\n",
    "# This will apply 40% structured pruning followed by unstructured pruning to reach ~70% total sparsity.\n",
    "prune_model(model, amount=0.7, method='mixed')\n",
    "##### Find \"weight_int\" for 6th layer ####\n",
    "w_bit = 4\n",
    "weight_q = model.features[17].weight_q\n",
    "w_alpha = model.features[17].weight_quant.wgt_alpha\n",
    "w_delta = w_alpha /(2**(w_bit-1)-1)\n",
    "\n",
    "weight_int = weight_q / w_delta\n",
    "# Show sparsity\n",
    "sparsity_weight_int = (weight_int == 0).sum() / weight_int.nelement()\n",
    "print(\"Sparsity level: \", sparsity_weight_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "weight_decay = 1e-6\n",
    "epochs = 1000\n",
    "best_prec = 0\n",
    "# momentum = 0.9\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1).cuda()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=epochs,  # 500\n",
    "    eta_min=1e-10,  # final LR\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
